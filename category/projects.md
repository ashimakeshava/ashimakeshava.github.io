---
layout: category
title: Projects
---

# _How do eye movements plan and guide actions in the natural world?_

### How does gaze support a sequence of actions?

<img src="../assets/public_images/ergovr.png" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
 Eye movements in the natural environment have primarily been studied for over-learned and habitual everyday activities (tea-making, sandwich-making, hand-washing) which have a fixed sequence of actions associated with them. These studies typically categorize eye movements related to low level action plans of locating the object of interest, directing the body or hand to the object, and monitoring the action execution. In this study, we were interested in generalizing these task-oriented eye movements for novel tasks that are not associated with an inherent action sequence. To that end, we asked subjects to sort objects based on object features on a life-size shelf in a virtual environment as we recorded their eye and body movements.
 <br/>
<a class="tag" href="https://www.biorxiv.org/content/10.1101/2021.01.29.428782v3" target="_blank">  Preprint  </a>
<a class="tag" href="https://github.com/ashimakeshava/ergovr_gaze_guidance" target="_blank">  Code  </a>
 <br/>
 <br/>

### What are the spatial biases in gaze behavior while interacting with tools?

<img src="../assets/public_images/GTI.png" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
Here we investigated active inference processes revealed by eye movements during interaction with familiar and novel tools with two levels of realism of interaction. We presented participants with 3D tool models that were either familiar or unfamiliar, oriented congruent or incongruent to their handedness, and asked participants to interact with them by lifting or using. Importantly, we used the same experimental design in two setups. In the first experiment, participants interacted with a VR controller; in the second, they performed the task with an interaction setup that allowed differentiated hand and finger movements. We investigated the differences in odds of fixations and their eccentricity towards the tool parts before action initiation.
<br/>
<a class="tag" href="https://doi.org/10.1111/ejn.15963" target="_blank" >  Paper  </a>
<a class="tag" href="https://github.com/ashimakeshava/gaze_tool_interaction" target="_blank" >  Code  </a>
<br/>
<br/>

### Can gaze behavior identify the task performed by the user?

<img src="../assets/public_images/vr_et_decoding.png" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
Here, we used simple gaze features such as proportion of fixations on regions of interest to classify a simple pick and place task performed by the user. We used SVMs with leave-one-subject out cross validation method to predict the task performed. Our results show that even simple gaze features are a robust signal and can successfully decode a userâ€™s task.
<br/>
<a class="tag" href="http://dx.doi.org/10.1145/3379156.3391338" target="_blank">  Paper  </a>
<br/>
<br/>

### How do eyes and hand coordinate to plan actions?

<img src="../assets/public_images/ergovr_ehc.png" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
Studies of eye-hand coordination are primarily conducted for sedentary tasks that do not require full-body movements. These tasks have shown that eye fixations precede manual action by ~1 second. However, we do not yet know how eyes and hands might coordinate in a larger spatial context where action locations vary and require coordination in different rotational planes.
<br/>
<a class="tag" href="https://www.biorxiv.org/content/10.1101/2024.03.30.587357v1" target="_blank">  Preprint  </a>
<a class="tag" href="https://osf.io/9edby/" target="_blank">  Code  </a>
<br/>
<br/>
### What is the neural basis of anticipatory gaze behavior?

<img src="../assets/public_images/GTI_EEG.png" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
Here, we use Generalized Eigen Decomposition to find the neural sources that are involved in active inference processes that lead to anticipatory gaze behavior.
<br/>
<a class="tag" href="">  In Prep  </a>
<br/>
<br/>

# Collab projects


### Is human-human collaboration different from human-robot collaboration?

<img src="../assets/public_images/hri.jpeg" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
<br/>
<a class="tag" href="http://dx.doi.org/10.3389/fnbot.2021.686010" target="_blank">  Paper  </a>
<a class="tag" href="https://osf.io/s6zbm/" target="_blank">  Code  </a>
<br/>
<br/>


### How do humans perceive autonomous driving vehicles?
<img src="../assets/public_images/westdrive.png" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
<br/>
<a class="tag" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9781604" target="_blank">  Paper  </a>

<br/>
<br/>

### How does embodied spatial exploration affect navigation abilities?
<img src="../assets/public_images/seahaven.jpeg" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
<br/>
<a class="tag" href="https://www.frontiersin.org/articles/10.3389/frvir.2021.625548/full" target="_blank">  Paper  </a>
<a class="tag" href="https://github.com/ashimakeshava/Seahaven_VR_Map_comparison" target="_blank">  Code  </a>
<br/>
<br/>


