---
layout: category
title: Projects
---

# _How do eye movements plan and guide actions in the natural world?_

### How does gaze support a sequence of actions?

<img src="../assets/public_images/ergovr.png" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
 Eye movements in the natural environment have primarily been studied for over-learned and habitual everyday activities (tea-making, sandwich-making, hand-washing) with a fixed sequence of associated actions. In this study, we were interested in how humans plan and execute actions for tasks that do not have an inherent action sequence. To that end, we asked subjects to sort objects based on object features on a life-size shelf in a virtual environment as we recorded their eye and body movements. We investigated the general characteristics of gaze behavior while acting under natural conditions. We provide a data-driven method of analyzing the different action-oriented functions of gaze. The results show that bereft of a predefined action sequence, humans prefer to plan only their immediate actions, where eye movements are used to search for the target object to immediately act on, then to guide the hand towards it and monitor the action until it is terminated. Such a simplistic approach ensures that humans choose sub-optimal behavior over planning under sustained cognitive load.
 <br/>
<a class="tag" href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1012529" target="_blank">  Paper  </a>
<a class="tag" href="https://osf.io/gq6aj/" target="_blank">  Code  </a>
 <br/>
 <br/>

### What are the spatial biases in gaze behavior while interacting with tools?

<img src="../assets/public_images/GTI.png" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
Here we investigated active inference processes revealed by eye movements during interaction with familiar and novel tools with two levels of realism of interaction. We presented participants with 3D tool models that were either familiar or unfamiliar, oriented congruent or incongruent to their handedness, and asked participants to interact with them by lifting or using. Importantly, we used the same experimental design in two setups. In the first experiment, participants interacted with a VR controller; in the second, they performed the task with an interaction setup that allowed differentiated hand and finger movements. We investigated the differences in odds of fixations and their eccentricity towards the tool parts before action initiation.
<br/>
<a class="tag" href="https://doi.org/10.1111/ejn.15963" target="_blank" >  Paper  </a>
<a class="tag" href="https://github.com/ashimakeshava/gaze_tool_interaction" target="_blank" >  Code  </a>
<br/>
<br/>

### Can gaze behavior identify the task performed by the user?

<img src="../assets/public_images/vr_et_decoding.png" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
Here, we used simple gaze features such as proportion of fixations on regions of interest to classify a simple pick and place task performed by the user. We used SVMs with leave-one-subject out cross validation method to predict the task performed. Our results show that even simple gaze features are a robust signal and can successfully decode a userâ€™s task.
<br/>
<a class="tag" href="http://dx.doi.org/10.1145/3379156.3391338" target="_blank">  Paper  </a>
<br/>
<br/>

### How do eyes and hand coordinate to plan actions?

<img src="../assets/public_images/ergovr_ehc.png" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
Studies of eye-hand coordination are primarily conducted for sedentary tasks that do not require full-body movements. These tasks have shown that eye fixations precede manual action by ~1 second. However, we do not yet know how eyes and hands might coordinate in a larger spatial context where action locations vary and require coordination in different rotational planes.
<br/>
<a class="tag" href="https://www.biorxiv.org/content/10.1101/2024.03.30.587357v1" target="_blank">  Preprint  </a>
<a class="tag" href="https://osf.io/9edby/" target="_blank">  Code  </a>
<br/>
<br/>
### What is the neural basis of anticipatory gaze behavior?

<img src="../assets/public_images/GTI_EEG.png" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
Here, we use Generalized Eigen Decomposition to find the neural sources that are involved in active inference processes that lead to anticipatory gaze behavior.
<br/>
<a class="tag" href="">  In Prep  </a>
<br/>
<br/>

# Collab projects

### What are the neural signatures of face perception at fixation onset?

<img src="../assets/public_images/hri.jpeg" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
<br/>
<a class="tag" href="http://dx.doi.org/10.3389/fnbot.2021.686010" target="_blank">  Paper  </a>
<a class="tag" href="https://osf.io/s6zbm/" target="_blank">  Code  </a>
<br/>
<br/>



### Is human-human collaboration different from human-robot collaboration?

<img src="../assets/public_images/hri.jpeg" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
<br/>
<a class="tag" href="http://dx.doi.org/10.3389/fnbot.2021.686010" target="_blank">  Paper  </a>
<a class="tag" href="https://osf.io/s6zbm/" target="_blank">  Code  </a>
<br/>
<br/>


### How do humans perceive autonomous driving vehicles?
<img src="../assets/public_images/westdrive.png" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
<br/>
<a class="tag" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9781604" target="_blank">  Paper  </a>

<br/>
<br/>

### How does embodied spatial exploration affect navigation abilities?
<img src="../assets/public_images/seahaven.jpeg" width="100%" style="padding:2px; float: right; margin-left:10px; margin-bottom:10px;"/>
<br/>
<a class="tag" href="https://www.frontiersin.org/articles/10.3389/frvir.2021.625548/full" target="_blank">  Paper  </a>
<a class="tag" href="https://github.com/ashimakeshava/Seahaven_VR_Map_comparison" target="_blank">  Code  </a>
<br/>
<br/>


